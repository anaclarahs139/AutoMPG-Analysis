ğŸï¸ AutoMPG-Analysis
ğŸ” Statistical analysis and fuel efficiency prediction (mpg) using linear regression and GAM models on the classic Auto dataset from StatLib (1983).

ğŸ“Š Dataset
The analysis is based on the Auto.csv dataset, available as part of the resources for the book:

James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013).
An Introduction to Statistical Learning with Applications in R, 2nd Edition.
Springer-Verlag, New York.
Available at: https://www.statlearning.com

ğŸ”— Dataset Source
ğŸ“š This dataset comes from the StatLib library, maintained by Carnegie Mellon University, and was originally featured in the 1983 ASA Exposition.

ğŸ“¥ You can download the .csv file directly from the Resources section of the ISLR book website:
ğŸŒ https://www.statlearning.com

ğŸ“„ Dataset Description
ğŸš— Observations: 392 vehicles
ğŸ“Š Features: 9 variables

ğŸ”  Variable	ğŸ“ Description
mpg	ğŸ“ˆ Miles per gallon (fuel efficiency)
cylinders	ğŸ”§ Number of cylinders (4â€“8)
displacement	ğŸ“¦ Engine displacement (cubic inches)
horsepower	ğŸ Engine horsepower
weight	âš–ï¸ Vehicle weight (lbs)
acceleration	ğŸ Time to accelerate from 0 to 60 mph (seconds)
year	ğŸ“† Model year (e.g. 70 = 1970)
origin	ğŸŒ Car origin: 1 = ğŸ‡ºğŸ‡¸ US, 2 = ğŸ‡ªğŸ‡º Europe, 3 = ğŸ‡¯ğŸ‡µ Japan
name	ğŸ·ï¸ Vehicle name (e.g. â€œchevrolet chevelle malibuâ€)

â„¹ï¸ Note: The original dataset included 397 entries.
ğŸ§¹ The ISLR2 version removes 5 rows due to missing horsepower values.
ğŸ”§ In this project, we handle missing values manually during data preprocessing.


ğŸ”½ 1. Data Import & Library Setup ğŸ“¦
Description: We begin by loading all necessary R libraries for data wrangling, visualization, and modeling. The dataset is imported from .csv with missing values handled explicitly.

ğŸ§° Key steps:

Load core packages: dplyr, ggplot2, mgcv, leaps, etc.

Import Auto.csv and flag "?" as NA.

ğŸ“‚ Output: A raw but structured DataFrame ready for cleaning and transformation.



ğŸ§¼ 2. Data Inspection & Cleaning ğŸ”
Description: We inspect variable classes, recast types where needed, and handle missing values in horsepower using group-wise mean imputation. We also identify and visualize outliers.

ğŸ› ï¸ Key steps:

Fix data types (e.g., origin â†’ factor, horsepower â†’ numeric)

Impute missing horsepower by cylinders & origin

Generate boxplot of horsepower after cleaning

ğŸ“ Insight: Missing values are smartly filled based on mechanical similarity. Outliers still sneak through â€” but we catch them.



ğŸ“Š 3. Exploratory Analysis of MPG
Description: We explore the distribution of mpg and compare fuel efficiency by region of origin (US, Europe, Japan). The dataset is then split into training and test subsets for modeling.

ğŸ”§ Key steps:

Boxplot & summary of mpg

Identify mpg outliers

Train-test split (75/25)

Visualize mpg by car origin

ğŸ§  Insight: Japanese cars generally achieve higher mpg, as expected. Time to let the models speak.



ğŸ”— 4. Correlation & Variable Relationships ğŸ“ˆ
Description: We calculate a correlation matrix of all numeric variables and visualize it using a color gradient. A custom upper-triangle scatter matrix shows nuanced relationships between variables.

ğŸ“Š Visualizations:

Heatmap of correlation coefficients

Colored scatterplots by model year (e.g., mpg vs. weight, horsepower, etc.)

ğŸ’¡ Insight: Weight and displacement are highly correlated â€” these big players drag mpg down. Year brings salvation.



ğŸ§ª 5. Baseline Linear Regression Modeling
Description: A multiple linear regression model is trained using weight, horsepower, year, and more. Predictions are made on the test set and compared to actual values.

ğŸ” Key metrics:

Predictions with 95% prediction intervals

Plot: actual vs. predicted mpg

Summary of residuals and model stats

ğŸ¯ Insight: Not perfect, but decent. Linear regression gives a baseline idea of how physical specs impact efficiency.



ğŸ“ 6. Model Performance Evaluation
Description: We compute standard performance metrics (MSE, RMSE, MAE, RÂ²) for both training and test sets to evaluate model generalization.

ğŸ“Š Metrics calculated:

Mean Squared Error (MSE)

Root Mean Squared Error (RMSE)

Mean Absolute Error (MAE)

R-squared (RÂ²)

ğŸ§  Insight: Some overfitting is visible â€” the model performs better on training data than unseen test data.



ğŸ› ï¸ 7. Polynomial & Interaction Modeling
Description: We improve the linear model by introducing polynomial terms and interaction effects. This allows us to capture non-linear dependencies and mixed-variable impacts.

ğŸ”§ Upgrades include:

poly(weight, 2), poly(displacement, 2)

Interactions: horsepower * year, weight * cylinders

ğŸ“‰ Insight: The enhanced model reduces error and boosts RÂ², but complexity increases. It's a trade-off â€” more flexibility, less interpretability.



ğŸ§® 8. Feature Selection via Subset Regression
Description: We apply exhaustive subset selection to find the optimal number of predictors using regsubsets(). Models are evaluated by Adjusted RÂ².

ğŸ“ˆ Visualization:

Line plot of Adjusted RÂ² vs. number of predictors

ğŸ¯ Insight: A 4â€“5 variable model often offers the best balance between simplicity and performance. More isnâ€™t always better.



ğŸŒ± 9. Nonlinear Trends with LOESS
Description: Before jumping into GAMs, we visualize the true shape of mpg relationships using LOESS smoothing. Each numeric predictor is plotted against mpg to capture subtle curves.

ğŸ§© Key features:

Smooth curves highlight nonlinear patterns

Visual cue for feature transformations

ğŸ’¡ Insight: mpg rises nonlinearly with year, drops sharply with weight. Time to call in the GAMs.



ğŸ”¥ 10. Generalized Additive Modeling (GAM)
Description: Using mgcv::gam(), we fit a flexible nonlinear model that lets each variable tell its own story. Spline terms (s(...)) are used to capture smooth trends.

ğŸ“Š Evaluation:

Actual vs. predicted mpg plots

Performance metrics (MSE, RMSE, RÂ², MAE) for both train and test sets

ğŸ Insight: GAM delivers strong, interpretable performance. It handles curvature naturally and provides the best generalization of all tested models.


ğŸ§  Final Takeaway:
This project shows how regression models â€” from linear baselines to nonlinear GAMs â€” can effectively decode the hidden patterns behind fuel efficiency. Combining visual exploration, feature selection, and flexible modeling makes for an insightful and powerful analysis.


